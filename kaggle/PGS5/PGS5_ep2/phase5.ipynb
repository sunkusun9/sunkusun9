{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3fcadb7-3a8b-4dcf-9885-a60e6a9fbd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 05:41:06.514803: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739166066.526883   44087 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739166066.530391   44087 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-10 05:41:06.542359: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.6 (main, Sep 30 2024, 02:19:13) [GCC 9.4.0]\n",
      "pandas 2.2.3\n",
      "polars 1.12.0\n",
      "matplotlib 3.8.4\n",
      "seaborn 0.13.2\n",
      "numpy 1.26.4\n",
      "scipy 1.12.0\n",
      "sklearn 1.5.2\n",
      "xgboost 2.1.2\n",
      "catboost 1.2.5\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "import dproc, sgml, sgnn, sgpp, sgutil, custpp\n",
    "\n",
    "print(sys.version)\n",
    "for i in [pd, pl, mpl, sns, np, scipy, sklearn, lgb, xgb, cb]:\n",
    "    try:\n",
    "        print(i.__name__, i.__version__)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66afb8c3-461e-4cbf-981a-042e3b876830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold, ShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18bb366e-3fcf-4f5a-980e-6da6f7860637",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = sgutil.SGCache('img', 'result')\n",
    "ss = ShuffleSplit(n_splits = 1, train_size = 0.8, random_state = 123)\n",
    "kf = KFold(5, random_state = 123, shuffle=True)\n",
    "\n",
    "files = {\n",
    "    'train': os.path.join('data', 'train.csv'),\n",
    "    'train_extra': os.path.join('data', 'training_extra.csv'),\n",
    "    'test': os.path.join('data', 'test.csv'),\n",
    "}\n",
    "\n",
    "t = sc.cache_result(\n",
    "    'pipeline_2',\n",
    "    lambda : make_pipeline(\n",
    "        sgpp.PolarsProcessor(), \n",
    "        sgpp.ExprProcessor({\n",
    "            'Compartments_c' : pl.col('Compartments').cast(pl.String).cast(pl.Categorical)\n",
    "        }),\n",
    "        sgpp.PandasCoverter(index_col = 'id'),\n",
    "        sgpp.ApplyWrapper(\n",
    "            sgpp.CatArrangerFreq(1, na_value = 'Unknown'),\n",
    "            ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "        ), \n",
    "        custpp.WeightCapacityProcessor()\n",
    "    ).fit(files['train']),\n",
    "    rerun = 1\n",
    ")\n",
    "df_train = pd.concat(\n",
    "    [t.transform(files['train']), t.transform(files['train_extra'])], axis = 0\n",
    ")\n",
    "df_test = t.transform(files['test'])\n",
    "\n",
    "target = 'Price'\n",
    "X_cat = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color', 'Compartments_c']\n",
    "X_num = ['Weight Capacity (kg)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2bd9de8-45f2-4f94-b418-f3d0c619f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_splitter(validation_fraction):\n",
    "    return lambda x: train_test_split(x, test_size = validation_fraction)\n",
    "\n",
    "config = {\n",
    "    'predict_func': lambda m, df, X: pd.Series(m.predict(df[X]), index = df.index).clip(15, 150),\n",
    "    'score_func': lambda df, prds: root_mean_squared_error(df[target].sort_index(), prds.sort_index()),\n",
    "    'validation_splitter': get_validation_splitter,\n",
    "    'progress_callback': sgml.ProgressCallBack(), \n",
    "    'return_train_scores': True,\n",
    "    'y': target,\n",
    "}\n",
    "\n",
    "cb_adapter = sgml.CBAdapter(cb.CatBoostRegressor)\n",
    "lr_adapter = sgml.SklearnAdapter(LinearRegression)\n",
    "lgb_adapter = sgml.LGBMAdapter(lgb.LGBMRegressor)\n",
    "xgb_adapter = sgml.XGBAdapter(xgb.XGBRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f176602-c6c0-42d9-9ddc-8fff7e3b748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.629543809953795 39.29101486311971 39.316704\n",
      "38.64572975855934 39.20963300258326 39.23759\n",
      "38.681409998918504 39.19875053182947 39.218143\n",
      "38.64570263604047 39.18592044966053 39.207096\n",
      "38.651380676657986 39.23936759986119 39.262196\n"
     ]
    }
   ],
   "source": [
    "tgt = TargetEncoder(smooth = 35, random_state = 123)\n",
    "for train_idx, valid_idx in kf.split(df_train, df_train[target]):\n",
    "    df_cv_train, df_valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    bidx = df_valid['Weight Capacity (kg)'].isin(df_cv_train['Weight Capacity (kg)'].unique()) & df_valid['Weight Capacity (kg)'].notna() &\\\n",
    "            df_valid['Weight Capacity (kg)'].between(5, 30, inclusive = 'neither')\n",
    "    df_valid, df_valid2 = df_valid.loc[bidx], df_valid.loc[~bidx]\n",
    "    tgt.fit(df_cv_train[['wc_i2']], df_cv_train[target].clip(df_cv_train[target].quantile(0.018), 150))\n",
    "    print(\n",
    "        root_mean_squared_error(df_valid[target], tgt.transform(df_valid[['wc_i2']])[:, 0]),\n",
    "        root_mean_squared_error(df_valid2[target], tgt.transform(df_valid2[['wc_i2']])[:, 0]),\n",
    "        df_valid2[target].std()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0024c6d2-bb1e-4570-b00f-ac8a1f064f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price\n",
       "150.000000    0.018046\n",
       "15.000000     0.002261\n",
       "39.834351     0.000049\n",
       "35.248440     0.000047\n",
       "77.643898     0.000046\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[target].value_counts(normalize = True).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a89288-8c45-42ce-b7e0-69610923d4cf",
   "metadata": {},
   "source": [
    "The fact that the frequencies of prices at 15 and 150 are high suggests that values have been clipped at 15 or below and at 150 or above. However, since the frequency of values at 15 or below is lower than that of values at 150 or above, the distribution exhibits an asymmetric shape. We examine the effect of adjusting the left-side clipping range of the target to match the extent of the right-side clipping, thereby achieving symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20995deb-4127-4ce7-85f9-ede7d06913f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.70549105651613 38.59238055257855 38.73538496482012 38.62967410652954 39.222756662786296 39.316704 0.8217593983456508\n"
     ]
    }
   ],
   "source": [
    "tgt = TargetEncoder(cv = 5, smooth = 35, target_type='continuous', random_state = 123)\n",
    "for train_idx, valid_idx in ss.split(df_train, df_train[target]):\n",
    "    df_cv_train, df_valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    df_cv_train = df_cv_train.assign(\n",
    "        Price = lambda x: x[target].clip(df_cv_train[target].quantile(0.018), 150),\n",
    "        tgt = lambda x: tgt.fit_transform(x[['wc_i2']], df_cv_train[target])[:, 0]\n",
    "    )\n",
    "    df_valid = df_valid\n",
    "    bidx = df_valid['Weight Capacity (kg)'].isin(df_cv_train['Weight Capacity (kg)'].unique()) & df_valid['Weight Capacity (kg)'].notna() &\\\n",
    "            df_valid['Weight Capacity (kg)'].between(5, 30, inclusive = 'neither')\n",
    "    df_valid1, df_valid2 = df_valid.loc[bidx], df_valid.loc[~bidx]\n",
    "    df_valid1 = df_valid1.assign(\n",
    "        tgt = lambda x: tgt.transform(x[['wc_i2']])[:, 0]\n",
    "    )\n",
    "    reg_cb = sc.cache_result(\n",
    "        'cb_ss_2',\n",
    "        lambda : sgml.train(df_cv_train, {\n",
    "                'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "                'X_num': X_num, 'X_cat': X_cat,\n",
    "                #'validation_fraction': 0.1\n",
    "            }, config, cb_adapter, task_type = 'GPU'), rerun = 0\n",
    "    )\n",
    "    reg_cb2 = sc.cache_result(\n",
    "        'cb2_ss_2',\n",
    "        lambda : sgml.train(df_cv_train, {\n",
    "                'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "                'X_cat': X_cat, 'X_num': ['tgt'],\n",
    "                # 'validation_fraction': 0.1\n",
    "            }, config, cb_adapter, task_type = 'GPU'), rerun = 0\n",
    "    )\n",
    "\n",
    "    s_merge = pd.concat([\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_valid1[reg_cb2[1]]), index = df_valid1.index\n",
    "        ),\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_valid2[reg_cb[1]]), index = df_valid2.index\n",
    "        )\n",
    "    ], axis = 0)\n",
    "    print(\n",
    "        root_mean_squared_error(df_valid[target].sort_index(), s_merge.sort_index()),\n",
    "        root_mean_squared_error(\n",
    "            df_valid1[target], make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_valid1[reg_cb2[1]])\n",
    "        ),\n",
    "        root_mean_squared_error(df_cv_train[target], df_cv_train['tgt']),\n",
    "        root_mean_squared_error(df_valid1[target], df_valid1['tgt']),\n",
    "        root_mean_squared_error(\n",
    "            df_valid2[target], make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_valid2[reg_cb[1]])\n",
    "        ), df_valid2[target].std(), bidx.mean()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d6addc2-7cee-4fb5-8e96-b80c941236d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cb = sc.cache_result(\n",
    "    'cb_2',\n",
    "    lambda : sgml.train(df_train.assign(Price = lambda x: x['Price'].clip(x['Price'].quantile(0.018), 150)), {\n",
    "            'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "            'X_num': X_num, 'X_cat': X_cat,\n",
    "            #'validation_fraction': 0.1\n",
    "        }, config, cb_adapter, task_type = 'GPU')\n",
    ")\n",
    "reg_cb2 = sc.cache_result(\n",
    "    'cb2_2',\n",
    "    lambda : sgml.train(df_train.assign(Price = lambda x: x['Price'].clip(x['Price'].quantile(0.018), 150)), {\n",
    "            'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "            'X_cat': X_cat, 'X_tgt': ['Weight Capacity (kg)'], 'tgt': {'cv': 5, 'smooth': 35, 'random_state': 123}\n",
    "            #'validation_fraction': 0.1\n",
    "        }, config, cb_adapter, task_type = 'GPU')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01767bb5-031a-4b5b-8d7d-1b7609bc9bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bidx = df_test['Weight Capacity (kg)'].isin(df_train['Weight Capacity (kg)'].unique()) & df_test['Weight Capacity (kg)'].notna() &\\\n",
    "            df_test['Weight Capacity (kg)'].between(5, 30, inclusive = 'neither')\n",
    "df_test1, df_test2 = df_test.loc[bidx], df_test.loc[~bidx]\n",
    "pd.concat([\n",
    "    pd.Series(\n",
    "        make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_test1[reg_cb2[1]]), index = df_test1.index\n",
    "    ),\n",
    "    pd.Series(\n",
    "        make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_test2[reg_cb[1]]), index = df_test2.index\n",
    "    )\n",
    "], axis = 0).rename(target).sort_index().to_frame().to_csv(os.path.join('result', 'submission4.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09aff2e3-a316-479c-a332-82823b4e345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LB: 38.91003\n",
    "#!kaggle competitions submit -c playground-series-s5e2 -f result/submission4.csv -m \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9d6d7ab-7022-463e-a325-b3f819c09423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.70549105651613 38.74319418961613 38.59238055257855 38.73538496482012 38.62967410652954 39.222756662786296 39.316704 0.8217593983456508\n"
     ]
    }
   ],
   "source": [
    "tgt = TargetEncoder(cv = 5, smooth = 35, target_type='continuous', random_state = 123)\n",
    "for train_idx, valid_idx in ss.split(df_train, df_train[target]):\n",
    "    df_cv_train, df_valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    df_cv_train = df_cv_train.assign(\n",
    "        Price = lambda x: x[target].clip(df_cv_train[target].quantile(0.018), 150),\n",
    "        tgt = lambda x: tgt.fit_transform(x[['wc_i2']], df_cv_train[target])[:, 0]\n",
    "    )\n",
    "    df_valid = df_valid.assign(\n",
    "        tgt = lambda x: tgt.transform(x[['wc_i2']])[:, 0]\n",
    "    )\n",
    "    bidx = df_valid['Weight Capacity (kg)'].isin(df_cv_train['Weight Capacity (kg)'].unique()) & df_valid['Weight Capacity (kg)'].notna() &\\\n",
    "            df_valid['Weight Capacity (kg)'].between(5, 30, inclusive = 'neither')\n",
    "    df_valid1, df_valid2 = df_valid.loc[bidx], df_valid.loc[~bidx]\n",
    "    reg_cb = sc.cache_result(\n",
    "        'cb_ss_2',\n",
    "        lambda : sgml.train(df_cv_train, {\n",
    "                'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "                'X_num': X_num, 'X_cat': X_cat,\n",
    "                #'validation_fraction': 0.1\n",
    "            }, config, cb_adapter, task_type = 'GPU'), rerun = 0\n",
    "    )\n",
    "    reg_cb2 = sc.cache_result(\n",
    "        'cb2_ss_2',\n",
    "        lambda : sgml.train(df_cv_train, {\n",
    "                'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "                'X_cat': X_cat, 'X_num': ['tgt'],\n",
    "                # 'validation_fraction': 0.1\n",
    "            }, config, cb_adapter, task_type = 'GPU'), rerun = 0\n",
    "    )\n",
    "\n",
    "    s_merge = pd.concat([\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_valid1[reg_cb2[1]]), index = df_valid1.index\n",
    "        ),\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_valid2[reg_cb[1]]), index = df_valid2.index\n",
    "        )\n",
    "    ], axis = 0)\n",
    "    s_add = (\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_valid[reg_cb2[1]]), index = df_valid.index\n",
    "        ) +\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_valid[reg_cb[1]]), index = df_valid.index\n",
    "        )\n",
    "    ) / 2\n",
    "    print(\n",
    "        root_mean_squared_error(df_valid[target].sort_index(), s_merge.sort_index()),\n",
    "        root_mean_squared_error(df_valid[target].sort_index(), s_add.sort_index()),\n",
    "        root_mean_squared_error(\n",
    "            df_valid1[target], make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_valid1[reg_cb2[1]])\n",
    "        ),\n",
    "        root_mean_squared_error(df_cv_train[target], df_cv_train['tgt']),\n",
    "        root_mean_squared_error(df_valid1[target], df_valid1['tgt']),\n",
    "        root_mean_squared_error(\n",
    "            df_valid2[target], make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_valid2[reg_cb[1]])\n",
    "        ), df_valid2[target].std(), bidx.mean()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe626a06-fc97-41e1-af65-e188297e2b2f",
   "metadata": {},
   "source": [
    "We used different models depending on whether Weight Capacity appeared only in the test set or if it had been clipped.\n",
    "\n",
    "This approach proved to be highly effective. Now, we aim to capture this characteristic using a single model.\n",
    "\n",
    "To achieve this, we define a variable for target encoding:\n",
    "Among the Weight Capacity (kg) values in the training set, those that appear in the test set—excluding 5 and 30, which resulted from clipping—are assigned specific values to apply the effect of target encoding.\n",
    "For all other cases, the values are set to 0, ensuring that target encoding is applied accordingly.\n",
    "\n",
    "We then verify whether the intended effect is achieved using a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21760c65-c7d5-4fb2-bb8a-5ba318f0e24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.69653651784812\n"
     ]
    }
   ],
   "source": [
    "tgt = TargetEncoder(cv = 5, smooth = 35, target_type='continuous', random_state = 123)\n",
    "for train_idx, valid_idx in ss.split(df_train, df_train[target]):\n",
    "    df_cv_train, df_valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    df_cv_train = df_cv_train.assign(\n",
    "        wc_i3 = lambda x: x['Weight Capacity (kg)'] * x['Weight Capacity (kg)'].isin(\n",
    "            df_valid['Weight Capacity (kg)'].pipe(lambda x: x.loc[x.between(5, 30, inclusive = 'neither')]).unique()\n",
    "        )\n",
    "    )\n",
    "    df_valid = df_valid.assign(\n",
    "        wc_i3 = lambda x: x['Weight Capacity (kg)'] * x['Weight Capacity (kg)'].isin(\n",
    "            df_cv_train['Weight Capacity (kg)'].pipe(lambda x: x.loc[x.between(5, 30, inclusive = 'neither')]).unique()\n",
    "        )\n",
    "    )\n",
    "    reg_cb = sc.cache_result(\n",
    "        'cb_ss_3',\n",
    "        lambda : sgml.train(df_cv_train, {\n",
    "                'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "                'X_num': X_num, 'X_cat': X_cat, 'X_tgt': ['wc_i3'], 'tgt': {'cv': 5, 'smooth': 35, 'random_state': 123}\n",
    "                #'validation_fraction': 0.1\n",
    "            }, config, cb_adapter, task_type = 'GPU'), rerun = 1\n",
    "    )\n",
    "    print(\n",
    "        root_mean_squared_error(df_valid[target],\n",
    "            make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_valid[reg_cb[1]])\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52adee-90b4-46c0-a6da-a4b72f501188",
   "metadata": {},
   "source": [
    "The effectiveness is observed. We examine its effectiveness by making a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "455ac409-3cab-4b08-8164-2f60f39fa1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03259083314850043, 0.2112469116291733)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_cv_train['wc_i3'] == 0).mean(), (df_valid['wc_i3'] == 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cf007-a8a2-4d4e-84a3-ce7951a6ad9f",
   "metadata": {},
   "source": [
    "# Submission 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a792d02-9a92-4093-a104-0df1061fc7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_cv_train.assign(\n",
    "    wc_i3 = lambda x: x['Weight Capacity (kg)'] * x['Weight Capacity (kg)'].isin(\n",
    "        df_test['Weight Capacity (kg)'].pipe(lambda x: x.loc[x.between(5, 30, inclusive = 'neither')]).unique()\n",
    "    )\n",
    ")\n",
    "df_test = df_test.assign(\n",
    "    wc_i3 = lambda x: x['Weight Capacity (kg)'] * x['Weight Capacity (kg)'].isin(\n",
    "        df_train['Weight Capacity (kg)'].pipe(lambda x: x.loc[x.between(5, 30, inclusive = 'neither')]).unique()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b60e8b7-a8a0-4af6-adb4-0e1433cc3d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cb = sc.cache_result(\n",
    "    'cb_3',\n",
    "    lambda : sgml.train(df_train, {\n",
    "            'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "            'X_num': X_num, 'X_cat': X_cat, 'X_tgt': ['wc_i3'], 'tgt': {'cv': 5, 'smooth': 35, 'random_state': 123}\n",
    "            #'validation_fraction': 0.1\n",
    "        }, config, cb_adapter, task_type = 'GPU'), rerun = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29e789c8-1661-4ad5-ae1d-b27baa840f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_test[reg_cb[1]]), index = df_test.index, name = target\n",
    ").sort_index().to_frame().to_csv(os.path.join('result', 'submission5.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e278e9d-da27-43f0-99ce-52e063576d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LB: 38.92596\n",
    "#!kaggle competitions submit -c playground-series-s5e2 -f result/submission5.csv -m \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b2616ea-a32a-426d-818e-08efc457aa7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41969873451471995, 0.17584)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_train['wc_i3'] == 0).mean(), (df_test['wc_i3'] == 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff93ab9-98cc-43cd-ad15-1e29b6d3eefd",
   "metadata": {},
   "source": [
    "On the leaderboard (LB), it actually showed a negative effect. Since the pattern of Weight Capacity differs from that during validation, the expected results were not achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87b3dfda-3281-4d62-99ce-1bc1def58cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.696492389789974 38.74008770455407 38.58149839302241 38.772034610161064 38.62967410652954 39.222299940278624 39.316704 0.8217593983456508\n"
     ]
    }
   ],
   "source": [
    "tgt = TargetEncoder(cv = 5, smooth = 35, target_type='continuous', random_state = 123)\n",
    "for train_idx, valid_idx in ss.split(df_train, df_train[target]):\n",
    "    df_cv_train, df_valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    df_cv_train = df_cv_train.assign(tgt = lambda x: tgt.fit_transform(x[['wc_i2']], df_cv_train[target])[:, 0])\n",
    "    df_valid = df_valid.assign(tgt = lambda x: tgt.transform(x[['wc_i2']])[:, 0])\n",
    "    bidx = df_valid['Weight Capacity (kg)'].isin(df_cv_train['Weight Capacity (kg)'].unique()) & df_valid['Weight Capacity (kg)'].notna() &\\\n",
    "            df_valid['Weight Capacity (kg)'].between(5, 30, inclusive = 'neither')\n",
    "    df_valid1, df_valid2 = df_valid.loc[bidx], df_valid.loc[~bidx]\n",
    "    reg_cb = sc.cache_result(\n",
    "        'cb_ss_4',\n",
    "        lambda : sgml.train(df_cv_train, {\n",
    "                'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "                'X_num': X_num, 'X_cat': X_cat,\n",
    "                #'validation_fraction': 0.1\n",
    "            }, config, cb_adapter, task_type = 'GPU'), rerun = 1\n",
    "    )\n",
    "    reg_cb2 = sc.cache_result(\n",
    "        'cb2_ss_4',\n",
    "        lambda : sgml.train(df_cv_train, {\n",
    "                'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "                'X_cat': X_cat, 'X_num': ['tgt', 'Weight Capacity (kg)'],\n",
    "                # 'validation_fraction': 0.1\n",
    "            }, config, cb_adapter, task_type = 'GPU'), rerun = 1\n",
    "    )\n",
    "\n",
    "    s_merge = pd.concat([\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_valid1[reg_cb2[1]]), index = df_valid1.index\n",
    "        ),\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_valid2[reg_cb[1]]), index = df_valid2.index\n",
    "        )\n",
    "    ], axis = 0)\n",
    "    s_add = (\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_valid[reg_cb2[1]]), index = df_valid.index\n",
    "        ) +\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_valid[reg_cb[1]]), index = df_valid.index\n",
    "        )\n",
    "    ) / 2\n",
    "    print(\n",
    "        root_mean_squared_error(df_valid[target].sort_index(), s_merge.sort_index()),\n",
    "        root_mean_squared_error(df_valid[target].sort_index(), s_add.sort_index()),\n",
    "        root_mean_squared_error(\n",
    "            df_valid1[target], make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_valid1[reg_cb2[1]])\n",
    "        ),\n",
    "        root_mean_squared_error(df_cv_train[target], df_cv_train['tgt']),\n",
    "        root_mean_squared_error(df_valid1[target], df_valid1['tgt']),\n",
    "        root_mean_squared_error(\n",
    "            df_valid2[target], make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_valid2[reg_cb[1]])\n",
    "        ), df_valid2[target].std(), bidx.mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af0298-60ed-490f-8479-7039636e5035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
