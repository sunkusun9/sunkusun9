{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3fcadb7-3a8b-4dcf-9885-a60e6a9fbd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 00:26:26.623273: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-09 00:26:26.901948: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-09 00:26:27.685128: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.3 (main, May  1 2024, 17:33:23) [GCC 11.4.0]\n",
      "pandas 2.2.2\n",
      "polars 1.12.0\n",
      "matplotlib 3.8.4\n",
      "seaborn 0.13.2\n",
      "numpy 1.26.4\n",
      "scipy 1.13.0\n",
      "sklearn 1.4.2\n",
      "lightgbm 4.3.0\n",
      "xgboost 2.1.2\n",
      "catboost 1.2.5\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "import dproc, sgml, sgnn, sgpp, sgutil, custpp\n",
    "\n",
    "print(sys.version)\n",
    "for i in [pd, pl, mpl, sns, np, scipy, sklearn, lgb, xgb, cb]:\n",
    "    try:\n",
    "        print(i.__name__, i.__version__)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66afb8c3-461e-4cbf-981a-042e3b876830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold, ShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18bb366e-3fcf-4f5a-980e-6da6f7860637",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = sgutil.SGCache('img', 'result')\n",
    "ss = ShuffleSplit(n_splits = 1, train_size = 0.8, random_state = 123)\n",
    "kf = KFold(5, random_state = 123, shuffle=True)\n",
    "\n",
    "files = {\n",
    "    'train': os.path.join('data', 'train.csv'),\n",
    "    'train_extra': os.path.join('data', 'training_extra.csv'),\n",
    "    'test': os.path.join('data', 'test.csv'),\n",
    "}\n",
    "\n",
    "t = sc.cache_result(\n",
    "    'pipeline_2',\n",
    "    lambda : make_pipeline(\n",
    "        sgpp.PolarsProcessor(), \n",
    "        sgpp.ExprProcessor({\n",
    "            'Compartments_c' : pl.col('Compartments').cast(pl.String).cast(pl.Categorical)\n",
    "        }),\n",
    "        sgpp.PandasCoverter(index_col = 'id'),\n",
    "        sgpp.ApplyWrapper(\n",
    "            sgpp.CatArrangerFreq(1, na_value = 'Unknown'),\n",
    "            ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "        ), \n",
    "        custpp.WeightCapacityProcessor()\n",
    "    ).fit(files['train']),\n",
    "    rerun = 1\n",
    ")\n",
    "df_train = pd.concat(\n",
    "    [t.transform(files['train']), t.transform(files['train_extra'])], axis = 0\n",
    ")\n",
    "df_test = t.transform(files['test'])\n",
    "\n",
    "target = 'Price'\n",
    "X_cat = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color', 'Compartments_c']\n",
    "X_num = ['Weight Capacity (kg)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2bd9de8-45f2-4f94-b418-f3d0c619f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_splitter(validation_fraction):\n",
    "    return lambda x: train_test_split(x, test_size = validation_fraction)\n",
    "\n",
    "config = {\n",
    "    'predict_func': lambda m, df, X: pd.Series(m.predict(df[X]), index = df.index).clip(15, 150),\n",
    "    'score_func': lambda df, prds: root_mean_squared_error(df[target].sort_index(), prds.sort_index()),\n",
    "    'validation_splitter': get_validation_splitter,\n",
    "    'progress_callback': sgml.ProgressCallBack(), \n",
    "    'return_train_scores': True,\n",
    "    'y': target,\n",
    "}\n",
    "\n",
    "cb_adapter = sgml.CBAdapter(cb.CatBoostRegressor)\n",
    "lr_adapter = sgml.SklearnAdapter(LinearRegression)\n",
    "lgb_adapter = sgml.LGBMAdapter(lgb.LGBMRegressor)\n",
    "xgb_adapter = sgml.XGBAdapter(xgb.XGBRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f176602-c6c0-42d9-9ddc-8fff7e3b748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.629543809953795 39.29101486311971 39.316704\n",
      "38.64572975855934 39.20963300258326 39.23759\n",
      "38.681409998918504 39.19875053182947 39.218143\n",
      "38.64570263604047 39.18592044966053 39.207096\n",
      "38.651380676657986 39.23936759986119 39.262196\n"
     ]
    }
   ],
   "source": [
    "tgt = TargetEncoder(smooth = 35, random_state = 123)\n",
    "for train_idx, valid_idx in kf.split(df_train, df_train[target]):\n",
    "    df_cv_train, df_valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    bidx = df_valid['Weight Capacity (kg)'].isin(df_cv_train['Weight Capacity (kg)'].unique()) & df_valid['Weight Capacity (kg)'].notna() &\\\n",
    "            df_valid['Weight Capacity (kg)'].between(5, 30, inclusive = 'neither')\n",
    "    df_valid, df_valid2 = df_valid.loc[bidx], df_valid.loc[~bidx]\n",
    "    tgt.fit(df_cv_train[['wc_i2']], df_cv_train[target].clip(df_cv_train[target].quantile(0.018), 150))\n",
    "    print(\n",
    "        root_mean_squared_error(df_valid[target], tgt.transform(df_valid[['wc_i2']])[:, 0]),\n",
    "        root_mean_squared_error(df_valid2[target], tgt.transform(df_valid2[['wc_i2']])[:, 0]),\n",
    "        df_valid2[target].std()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0024c6d2-bb1e-4570-b00f-ac8a1f064f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price\n",
       "150.000000    57556\n",
       "15.000000      7211\n",
       "39.834351       157\n",
       "27.485460       154\n",
       "38.375099       154\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv_train[target].value_counts().iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20995deb-4127-4ce7-85f9-ede7d06913f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.7053771769684 38.59229795035566 38.73538496482012 38.62967410652954 39.22250088744373 39.316704 0.8217593983456508\n"
     ]
    }
   ],
   "source": [
    "tgt = TargetEncoder(cv = 5, smooth = 35, target_type='continuous', random_state = 123)\n",
    "for train_idx, valid_idx in ss.split(df_train, df_train[target]):\n",
    "    df_cv_train, df_valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    df_cv_train = df_cv_train.assign(\n",
    "        Price = lambda x: x[target].clip(df_cv_train[target].quantile(0.018), 150),\n",
    "        tgt = lambda x: tgt.fit_transform(x[['wc_i2']], df_cv_train[target])[:, 0]\n",
    "    )\n",
    "    df_valid = df_valid\n",
    "    bidx = df_valid['Weight Capacity (kg)'].isin(df_cv_train['Weight Capacity (kg)'].unique()) & df_valid['Weight Capacity (kg)'].notna() &\\\n",
    "            df_valid['Weight Capacity (kg)'].between(5, 30, inclusive = 'neither')\n",
    "    df_valid1, df_valid2 = df_valid.loc[bidx], df_valid.loc[~bidx]\n",
    "    df_valid1 = df_valid1.assign(\n",
    "        tgt = lambda x: tgt.transform(x[['wc_i2']])[:, 0]\n",
    "    )\n",
    "    reg_cb = sc.cache_result(\n",
    "        'cb_ss_2',\n",
    "        lambda : sgml.train(df_cv_train, {\n",
    "                'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "                'X_num': X_num, 'X_cat': X_cat,\n",
    "                #'validation_fraction': 0.1\n",
    "            }, config, cb_adapter, task_type = 'GPU'), rerun = 0\n",
    "    )\n",
    "    reg_cb2 = sc.cache_result(\n",
    "        'cb2_ss_2',\n",
    "        lambda : sgml.train(df_cv_train, {\n",
    "                'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "                'X_cat': X_cat, 'X_num': ['tgt'],\n",
    "                # 'validation_fraction': 0.1\n",
    "            }, config, cb_adapter, task_type = 'GPU'), rerun = 1\n",
    "    )\n",
    "\n",
    "    s_merge = pd.concat([\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_valid1[reg_cb2[1]]), index = df_valid1.index\n",
    "        ),\n",
    "        pd.Series(\n",
    "            make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_valid2[reg_cb[1]]), index = df_valid2.index\n",
    "        )\n",
    "    ], axis = 0)\n",
    "    print(\n",
    "        root_mean_squared_error(df_valid[target].sort_index(), s_merge.sort_index()),\n",
    "        root_mean_squared_error(\n",
    "            df_valid1[target], make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_valid1[reg_cb2[1]])\n",
    "        ),\n",
    "        root_mean_squared_error(df_cv_train[target], df_cv_train['tgt']),\n",
    "        root_mean_squared_error(df_valid1[target], df_valid1['tgt']),\n",
    "        root_mean_squared_error(\n",
    "            df_valid2[target], make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_valid2[reg_cb[1]])\n",
    "        ), df_valid2[target].std(), bidx.mean()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d6addc2-7cee-4fb5-8e96-b80c941236d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cb = sc.cache_result(\n",
    "    'cb_2',\n",
    "    lambda : sgml.train(df_train.assign(Price = lambda x: x['Price'].clip(x['Price'].quantile(0.018), 150)), {\n",
    "            'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "            'X_num': X_num, 'X_cat': X_cat,\n",
    "            #'validation_fraction': 0.1\n",
    "        }, config, cb_adapter, task_type = 'GPU')\n",
    ")\n",
    "reg_cb2 = sc.cache_result(\n",
    "    'cb2_2',\n",
    "    lambda : sgml.train(df_train.assign(Price = lambda x: x['Price'].clip(x['Price'].quantile(0.018), 150)), {\n",
    "            'model_params' : {'n_estimators': 500, 'learning_rate': 0.1},\n",
    "            'X_cat': X_cat, 'X_tgt': ['Weight Capacity (kg)'], 'tgt': {'cv': 5, 'smooth': 35, 'random_state': 123}\n",
    "            #'validation_fraction': 0.1\n",
    "        }, config, cb_adapter, task_type = 'GPU')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01767bb5-031a-4b5b-8d7d-1b7609bc9bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bidx = df_test['Weight Capacity (kg)'].isin(df_train['Weight Capacity (kg)'].unique()) & df_test['Weight Capacity (kg)'].notna() &\\\n",
    "            df_test['Weight Capacity (kg)'].between(5, 30, inclusive = 'neither')\n",
    "df_test1, df_test2 = df_test.loc[bidx], df_test.loc[~bidx]\n",
    "pd.concat([\n",
    "    pd.Series(\n",
    "        make_pipeline(reg_cb2[0]['preprocessor'], reg_cb2[0]['model']).predict(df_test1[reg_cb2[1]]), index = df_test1.index\n",
    "    ),\n",
    "    pd.Series(\n",
    "        make_pipeline(reg_cb[0]['preprocessor'], reg_cb[0]['model']).predict(df_test2[reg_cb[1]]), index = df_test2.index\n",
    "    )\n",
    "], axis = 0).rename(target).sort_index().to_frame().to_csv(os.path.join('result', 'submission4.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09aff2e3-a316-479c-a332-82823b4e345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LB: 38.91003\n",
    "#!kaggle competitions submit -c playground-series-s5e2 -f result/submission4.csv -m \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6d7ab-7022-463e-a325-b3f819c09423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
