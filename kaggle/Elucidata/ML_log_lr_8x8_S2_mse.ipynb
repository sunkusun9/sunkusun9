{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ca6e1c-6c6c-40b2-afa8-a36faf0cf557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 00:53:11.300021: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-06 00:53:11.310088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746492791.321818     297 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746492791.325351     297 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-06 00:53:11.339010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import dproc, sgutil, sgpp, sgnn, sgml\n",
    "import warnings\n",
    "\n",
    "# 모든 FutureWarning 무시\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2e83d-0d6f-4efc-bed0-2cb3967d86e9",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9d49ae-5b81-440d-9553-2b3c157d9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    이미지를 불러옵니다.\n",
    "    Parameters:\n",
    "        filename: str\n",
    "            h5 파일에서 데이터를 불러옵니다.\n",
    "    Returns:\n",
    "        np.ndarray, pd.DataFrame, np.ndarray, \n",
    "        train 이미지, train spot 정보, test 이미지, test spot 정보\n",
    "    \"\"\"\n",
    "    images, images_test = list(), list()\n",
    "    spots, spots_test = list(), list()\n",
    "    with h5py.File(filename, \"r\") as h5file:\n",
    "        train_images = h5file[\"images/Train\"]\n",
    "        train_spots = h5file[\"spots/Train\"]\n",
    "    \n",
    "        num_train_slides = len(train_images)\n",
    "        # Train 이미지를 불러옵니다.\n",
    "        # 하나의 텐서로 만들기 위해 이미지의 크기를 2000x2000으로 균일하게 만듭니다.\n",
    "        for i, slide_name in enumerate(train_images.keys()):\n",
    "            image = np.array(train_images[slide_name])\n",
    "            p1 = 2000 - image.shape[0]\n",
    "            p2 = 2000 - image.shape[1]\n",
    "            images.append(\n",
    "                np.pad(image, [(0, p1), (0, p2), (0, 0)], 'edge')\n",
    "            )\n",
    "            spots.append(pd.DataFrame(np.array(train_spots[slide_name])).assign(slide = i))\n",
    "            if slide_name == 'S_2':\n",
    "                spots[-1] = spots[-1].assign(\n",
    "                    x = lambda x: x['x'] - 60,\n",
    "                    y = lambda x: x['y'] - 60,\n",
    "                )\n",
    "        # Test 이미지를 불러옵니다.\n",
    "        test_images = h5file[\"images/Test\"]\n",
    "        test_spots = h5file[\"spots/Test\"]\n",
    "        sample = 'S_7'\n",
    "        image = np.array(test_images[sample])\n",
    "        p1 = 2000 - image.shape[0]\n",
    "        p2 = 2000 - image.shape[1]\n",
    "        images_test.append(np.pad(image, [(0, p1), (0, p2), (0, 0)], 'edge'))\n",
    "        spots_test.append(pd.DataFrame(np.array(test_spots[sample])).assign(slide = 0))\n",
    "        df_spots = pd.concat(spots).reset_index(drop = True)\n",
    "        images = np.stack(images)\n",
    "        images_test = np.stack(images_test)\n",
    "        df_spots_test = pd.concat(spots_test).reset_index(drop = True)\n",
    "    return images, df_spots, images_test, df_spots_test\n",
    "\n",
    "def make_img_proc_info(df, img_width, img_height):\n",
    "    return df.assign(\n",
    "        left = lambda x: (x['x'] - img_width // 2).astype('int'),\n",
    "        right = lambda x: (x['left'] + img_width).astype('int'),\n",
    "        top = lambda x: (x['y'] - img_height // 2).astype('int'),\n",
    "        bottom = lambda x: (x['top'] + img_height).astype('int'),\n",
    "        lpad = lambda x: -(x['left'].where(x['left'] < 0, 0)),\n",
    "        rpad = lambda x: -(2000 - x['right']).where(x['right'] > 2000, 0),\n",
    "        tpad = lambda x: -(x['top'].where(x['top'] < 0, 0)),\n",
    "        bpad = lambda x: -(2000 - x['bottom']).where(x['bottom'] > 2000, 0)\n",
    "    ).assign(\n",
    "        left = lambda x: x['left'].clip(0, 2000),\n",
    "        right = lambda x: x['right'].clip(0, 2000),\n",
    "        top = lambda x: x['top'].clip(0, 2000),\n",
    "        bottom = lambda x: x['bottom'].clip(0, 2000),\n",
    "    )\n",
    "\n",
    "def create_df(df, img_width, img_height):\n",
    "    df = make_img_proc_info(df, img_width, img_height)\n",
    "    df_pixel = df[['left', 'right', 'top', 'bottom', 'slide', 'lpad', 'rpad', 'tpad', 'bpad']].apply(\n",
    "        lambda x: pd.Series(proc_images_np(x, images)), axis = 1\n",
    "    ).rename(columns = lambda x: 'pixel_{}'.format(x)).reset_index(drop = True)\n",
    "    X_pixel = df_pixel.columns\n",
    "    return df.join(df_pixel), X_pixel\n",
    "\n",
    "def proc_images_np(X, images):\n",
    "    return np.pad(\n",
    "        images[X['slide'], X['left']:X['right'], X['top']:X['bottom'], :], \n",
    "        [(X['lpad'], X['rpad']), (X['tpad'], X['bpad']), (0, 0)], 'edge'\n",
    "    ).flatten()\n",
    "\n",
    "\n",
    "images, df_spots, images_test, df_spots_test = load_data(\"data/elucidata_ai_challenge_data.h5\")\n",
    "targets = [i for i in df_spots.columns if i.startswith('C')]\n",
    "n_components = 35\n",
    "targets2 = ['C{}_l'.format(i + 1) for i in range(n_components)]\n",
    "df_spots= df_spots.join(\n",
    "    np.log(df_spots[targets]).rename(columns = lambda x: x + '_l')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d99564ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, train_test_split\n",
    "\n",
    "gkf = GroupKFold(6)\n",
    "ss = GroupShuffleSplit(1)\n",
    "sc = sgutil.SGCache('img', 'result', 'model')\n",
    "\n",
    "def get_validation_splitter(validation_fraction):\n",
    "    return lambda x: train_test_split(x, test_size = validation_fraction)\n",
    "\n",
    "def spearman(df, df_prds):\n",
    "    return df_prds.apply(\n",
    "        lambda x: spearmanr(x, df.loc[x.name, targets])[0],axis=1\n",
    "    ).mean()\n",
    "\n",
    "config = {\n",
    "    'predict_func': lambda m, df, X: pd.DataFrame(np.exp(m.predict(df[X])), index = df.index, columns = targets2),\n",
    "    'score_func': lambda df, prds: spearman(df[targets], prds),\n",
    "    'validation_splitter': get_validation_splitter,\n",
    "    'progress_callback': sgml.ProgressCallBack(), \n",
    "    'return_train_scores': True,\n",
    "    'y': targets2, 'groups': 'slide'\n",
    "}\n",
    "\n",
    "\n",
    "xgb_adapter = sgml.XGBAdapter(xgb.XGBRegressor, progress = 50)\n",
    "lasso_adapter = sgml.SklearnAdapter(Lasso)\n",
    "ridge_adapter = sgml.SklearnAdapter(Ridge)\n",
    "mlp_adapter = sgml.SklearnAdapter(MLPRegressor)\n",
    "\n",
    "lr_adapter = sgml.SklearnAdapter(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d65713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spots8, X_pixel8 = create_df(df_spots, 8, 8)\n",
    "df_spots9, X_pixel9 = create_df(df_spots, 9, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59947acf",
   "metadata": {},
   "source": [
    "# Linear Regression1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68cc41e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.45159830844446774,\n",
       " [0.5297622132553462,\n",
       "  0.35523506124946186,\n",
       "  0.285001394714117,\n",
       "  0.4690515127702302,\n",
       "  0.5735355011569846,\n",
       "  0.4970041675206669])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_lr = sc.cv_result(\n",
    "    'LR', df_spots8, gkf, {'X_num': X_pixel8.tolist()}, config, lr_adapter, result_proc = [sgml.lr_learning_result]\n",
    ")\n",
    "np.mean(result_lr['valid_scores']), result_lr['valid_scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990c082",
   "metadata": {},
   "source": [
    "#  XGBoost1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8feddcee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.45333574818398664,\n",
       " [0.5581417589062058,\n",
       "  0.3278171532609403,\n",
       "  0.2877363706365074,\n",
       "  0.44514948355079187,\n",
       "  0.5775053789631795,\n",
       "  0.523664343786295])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = {\n",
    "    'model_params': {'max_depth': 2, 'learning_rate': 0.01, 'n_estimators': 100},\n",
    "    'X_num': X_pixel8.tolist()\n",
    "}\n",
    "result_xgb = sc.cv_result('xgb', df_spots8, gkf, hparams, config, xgb_adapter)\n",
    "np.mean(result_xgb['valid_scores']), result_xgb['valid_scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576785e",
   "metadata": {},
   "source": [
    "# Linear Regression 2\n",
    "\n",
    "- Linear Regression + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b3ff6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4785741677274913,\n",
       " [0.585518577475043,\n",
       "  0.3823903667829625,\n",
       "  0.2811122302230374,\n",
       "  0.47779563383904533,\n",
       "  0.6224199244915357,\n",
       "  0.5222082735533239])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = {'pca': {'X_num': X_pixel8.tolist(), 'hparams': {'n_components': 0.7}}}\n",
    "result_lr2 = sc.cv_result(\n",
    "    'LR2', df_spots8, gkf, hparams, config, lr_adapter, result_proc = [sgml.lr_learning_result]\n",
    ")\n",
    "np.mean(result_lr2['valid_scores']), result_lr2['valid_scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509930b0",
   "metadata": {},
   "source": [
    "# Linear Regression 3\n",
    "\n",
    "- Linear Regression + PCA + pixel9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ae9911b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4788232912064534,\n",
       " [0.5861936845222671,\n",
       "  0.38231418193130695,\n",
       "  0.283400730596353,\n",
       "  0.47704874704726036,\n",
       "  0.6214675435391548,\n",
       "  0.5225148596023776])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = {'pca': {'X_num': X_pixel9.tolist(), 'hparams': {'n_components': 0.5}}}\n",
    "result_lr3 = sc.cv_result(\n",
    "    'LR3', df_spots9, gkf, hparams, config, lr_adapter, result_proc = [sgml.lr_learning_result], rerun = 0\n",
    ")\n",
    "np.mean(result_lr3['valid_scores']), result_lr3['valid_scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1747c18f",
   "metadata": {},
   "source": [
    "# Ridge1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c47a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4953370298393036,\n",
       " [0.5607180493342712,\n",
       "  0.44186317221472116,\n",
       "  0.27515070428887123,\n",
       "  0.4832064923694837,\n",
       "  0.6716981285267731,\n",
       "  0.5393856323017012])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = {'X_num': X_pixel8.tolist(), 'model_params': {'alpha': 1e7}}\n",
    "result_rd1 = sc.cv_result(\n",
    "    'Ridge1', df_spots8, gkf, hparams, config, ridge_adapter, result_proc = [sgml.lr_learning_result], rerun = 0\n",
    ")\n",
    "np.mean(result_rd1['valid_scores']), result_rd1['valid_scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4a55a",
   "metadata": {},
   "source": [
    "# Lasso1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9895530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49562809082597,\n",
       " [0.5607180493342712,\n",
       "  0.44186317221472116,\n",
       "  0.27515070428887123,\n",
       "  0.4832064923694837,\n",
       "  0.6734624284496407,\n",
       "  0.5393676982988318])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = {'X_num': X_pixel8.tolist(), 'model_params': {'alpha': 10}}\n",
    "result_lasso1 = sc.cv_result(\n",
    "    'lasso1', df_spots8, gkf, hparams, config, lasso_adapter, result_proc = [sgml.lr_learning_result], rerun = 0\n",
    ")\n",
    "np.mean(result_lasso1['valid_scores']), result_lasso1['valid_scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749e4be5",
   "metadata": {},
   "source": [
    "# MLP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e2c8cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49370322399374444,\n",
       " [0.5401794197855354,\n",
       "  0.44183474026843333,\n",
       "  0.27287523238275635,\n",
       "  0.48265547162420147,\n",
       "  0.6751305159745059,\n",
       "  0.5495439639270343])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = {'X_num': X_pixel8.tolist(), 'model_params': {'alpha': 1e4, 'max_iter': 1000, 'hidden_layer_sizes' : (16, )}}\n",
    "result_mlp1 = sc.cv_result(\n",
    "    'mlp1', df_spots8, gkf, hparams, config, mlp_adapter, rerun = 0\n",
    ")\n",
    "np.mean(result_mlp1['valid_scores']), result_mlp1['valid_scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff4170f",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7e6144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd = np.zeros_like(df_spots[targets])\n",
    "for i in [\n",
    "    #sc.read_prd('ridge1', df_spots.index, columns = targets),\n",
    "    sc.read_prd('lasso1', df_spots.index, columns = targets),\n",
    "    #sc.read_prd('mlp1', df_spots.index, columns = targets),\n",
    "]:\n",
    "    prd += i[targets].rank(axis = 1)\n",
    "\n",
    "spearman(\n",
    "    df_spots[targets],\n",
    "    pd.DataFrame(prd, index = df_spots.index, columns = targets)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e39ee98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46947160268238924"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eab58473-e4a2-4138-9981-0ded6a2f548b",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0a9b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sc.train_cv('lasso1', df_spots8, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03886d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
