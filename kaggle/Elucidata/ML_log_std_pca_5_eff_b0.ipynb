{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ca6e1c-6c6c-40b2-afa8-a36faf0cf557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "# 모든 FutureWarning 무시\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2e83d-0d6f-4efc-bed0-2cb3967d86e9",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9d49ae-5b81-440d-9553-2b3c157d9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    이미지를 불러옵니다.\n",
    "    Parameters:\n",
    "        filename: str\n",
    "            h5 파일에서 데이터를 불러옵니다.\n",
    "    Returns:\n",
    "        np.ndarray, pd.DataFrame, np.ndarray, \n",
    "        train 이미지, train spot 정보, test 이미지, test spot 정보\n",
    "    \"\"\"\n",
    "    images, images_test = list(), list()\n",
    "    spots, spots_test = list(), list()\n",
    "    with h5py.File(filename, \"r\") as h5file:\n",
    "        train_images = h5file[\"images/Train\"]\n",
    "        train_spots = h5file[\"spots/Train\"]\n",
    "    \n",
    "        num_train_slides = len(train_images)\n",
    "        # Train 이미지를 불러옵니다.\n",
    "        # 하나의 텐서로 만들기 위해 이미지의 크기를 2000x2000으로 균일하게 만듭니다.\n",
    "        for i, slide_name in enumerate(train_images.keys()):\n",
    "            image = np.array(train_images[slide_name])\n",
    "            p1 = 2000 - image.shape[0]\n",
    "            p2 = 2000 - image.shape[1]\n",
    "            images.append(\n",
    "                np.pad(image, [(0, p1), (0, p2), (0, 0)], 'edge')\n",
    "            )\n",
    "            spots.append(pd.DataFrame(np.array(train_spots[slide_name])).assign(slide = i))\n",
    "        # Test 이미지를 불러옵니다.\n",
    "        test_images = h5file[\"images/Test\"]\n",
    "        test_spots = h5file[\"spots/Test\"]\n",
    "        sample = 'S_7'\n",
    "        image = np.array(test_images[sample])\n",
    "        p1 = 2000 - image.shape[0]\n",
    "        p2 = 2000 - image.shape[1]\n",
    "        images_test.append(np.pad(image, [(0, p1), (0, p2), (0, 0)], 'edge'))\n",
    "        spots_test.append(pd.DataFrame(np.array(test_spots[sample])).assign(slide = 0))\n",
    "    # EfficientNet의 형식으로 바꿉니다.\n",
    "    with tf.device('/CPU:0'):\n",
    "        images = tf.constant(tf.keras.applications.efficientnet.preprocess_input(images))\n",
    "    df_spots = pd.concat(spots).reset_index(drop = True)\n",
    "    with tf.device('/CPU:0'):\n",
    "        images_test = tf.constant(tf.keras.applications.efficientnet.preprocess_input(images_test))\n",
    "    df_spots_test = pd.concat(spots_test).reset_index(drop = True)\n",
    "    return images, df_spots, images_test, df_spots_test\n",
    "\n",
    "def make_img_proc_info(df, img_with, img_height):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return df.assign(\n",
    "        left = lambda x: x['x'] - img_width // 2,\n",
    "        right = lambda x: x['x'] + img_width // 2,\n",
    "        top = lambda x: x['y'] - img_height // 2,\n",
    "        bottom = lambda x: x['y'] + img_height // 2,\n",
    "        lpad = lambda x: -(x['left'].where(x['left'] < 0, 0)),\n",
    "        rpad = lambda x: -(2000 - x['right']).where(x['right'] > 2000, 0),\n",
    "        tpad = lambda x: -(x['top'].where(x['top'] < 0, 0)),\n",
    "        bpad = lambda x: -(2000 - x['bottom']).where(x['bottom'] > 2000, 0)\n",
    "    ).assign(\n",
    "        left = lambda x: x['left'].clip(0, 2000),\n",
    "        right = lambda x: x['right'].clip(0, 2000),\n",
    "        top = lambda x: x['top'].clip(0, 2000),\n",
    "        bottom = lambda x: x['bottom'].clip(0, 2000),\n",
    "    )\n",
    "\n",
    "def create_tf_ds(df):\n",
    "    if (pd.Series(targets).isin(df.columns)).all():\n",
    "        return tf.data.Dataset.from_tensor_slices(\n",
    "            ({\n",
    "                i: df[i] for i in ['left', 'right', 'top', 'bottom', 'slide', 'lpad', 'rpad', 'tpad', 'bpad']\n",
    "            }, df[targets2])\n",
    "        )\n",
    "    else:\n",
    "        return tf.data.Dataset.from_tensor_slices({\n",
    "            i: df[i] for i in ['left', 'right', 'top', 'bottom', 'slide', 'lpad', 'rpad', 'tpad', 'bpad']\n",
    "        })\n",
    "\n",
    "def proc_images(X, images):\n",
    "    return tf.pad(\n",
    "        images[X['slide'], X['left']:X['right'], X['top']:X['bottom'], :], \n",
    "        paddings = [(X['lpad'], X['rpad']), (X['tpad'], X['bpad']), (0, 0)],\n",
    "        constant_values=1\n",
    "    )\n",
    "\n",
    "augmentation_layers = [\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.5),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1)\n",
    "]\n",
    "\n",
    "def data_augmentation(x):\n",
    "    for layer in augmentation_layers:\n",
    "        x = layer(x)\n",
    "    return x\n",
    "\n",
    "images, df_spots, images_test, df_spots_test = load_data(\"data/elucidata_ai_challenge_data.h5\")\n",
    "targets = [i for i in df_spots.columns if i.startswith('C')]\n",
    "n_components = 5\n",
    "target_proc = make_pipeline(\n",
    "    FunctionTransformer(np.log, np.exp), StandardScaler(), PCA(n_components=n_components)\n",
    ").fit(df_spots[targets])\n",
    "target_proc.fit(df_spots[targets])\n",
    "targets2 = ['pca_{}'.format(i) for i in range(n_components)]\n",
    "df_spots= df_spots.join(\n",
    "    pd.DataFrame(target_proc.transform(df_spots[targets]), index = df_spots.index, columns = targets2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08aa61c-806d-4c04-b228-132bd9491c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spots['slide'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f6bb94-80cf-4636-956d-26eb52785bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def create_model(img_width, img_height):\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "    enet = tf.keras.applications.EfficientNetB0(\n",
    "        include_top = False, \n",
    "        weights = 'imagenet',\n",
    "        input_shape = input_shape,\n",
    "        pooling = 'avg'\n",
    "    )\n",
    "    inputs = tf.keras.Input(shape = input_shape)\n",
    "    x = enet(inputs, training = False)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    d1 = tf.keras.layers.Dense(256, activation = 'relu', kernel_initializer = 'HeUniform')\n",
    "    x = d1(x)\n",
    "    d2 = tf.keras.layers.Dense(len(targets2))\n",
    "    outputs = d2(x)\n",
    "    m = tf.keras.models.Model(inputs, outputs)\n",
    "    return m, (enet, d1, d2)\n",
    "\n",
    "def reconstruct_model(layers):\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "    inputs = tf.keras.Input(shape = input_shape)\n",
    "    x = layers[0](inputs, training = True)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = layers[1](x)\n",
    "    outputs = layers[2](x)\n",
    "    m = tf.keras.models.Model(inputs, outputs)\n",
    "    return m\n",
    "\n",
    "def train_model(\n",
    "        m, train_idx, valid_idx, learning_rate, \n",
    "        target_proc = FunctionTransformer(lambda x: x, lambda x: x), \n",
    "        batch_size = 32, epochs = 20, step = ''\n",
    "    ):\n",
    "\n",
    "    ds_cv_train = create_tf_ds(\n",
    "        df_spots.iloc[train_idx].pipe(\n",
    "            lambda x: pd.concat([\n",
    "                x, x.sample(n = batch_size - (len(x) % batch_size))\n",
    "            ])\n",
    "        )\n",
    "    ).shuffle(5000).map(\n",
    "        lambda X, Y: (proc_images(X, images), Y)\n",
    "    ).map(\n",
    "        lambda X, Y: (data_augmentation(X), Y)\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "    ds_cv_prd = create_tf_ds(\n",
    "        df_spots.iloc[train_idx]\n",
    "    ).map(\n",
    "        lambda X, Y: (proc_images(X, images), Y)\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE).cache()\n",
    "    \n",
    "    if valid_idx is not None:\n",
    "        ds_valid = create_tf_ds(df_spots.iloc[valid_idx]).map(\n",
    "            lambda X, Y: (proc_images(X, images), Y)\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE).cache()\n",
    "    else:\n",
    "        ds_valid = None\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=learning_rate,\n",
    "        decay_steps=3000,\n",
    "        alpha=0.1\n",
    "    )\n",
    "#    lr_schedule = learning_rate\n",
    "    m.compile(\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n",
    "        metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    "    )\n",
    "\n",
    "    df_true_train = df_spots.iloc[train_idx][targets]\n",
    "    if valid_idx is not None:\n",
    "        df_true = df_spots.iloc[valid_idx][targets]\n",
    "    else:\n",
    "        df_true = None\n",
    "    progress_bar = tqdm(total = epochs, desc=step)\n",
    "    scores_train, scores_valid = list(), list()\n",
    "    df_prd = None\n",
    "    for i in range(epochs):\n",
    "        hist = m.fit(ds_cv_train, epochs = 1, verbose = 0)\n",
    "        df_prd = pd.DataFrame(\n",
    "            target_proc.inverse_transform(m.predict(ds_cv_prd, verbose = 0)), \n",
    "            index = df_true_train.index, columns = targets\n",
    "        )\n",
    "        scores_train.append(\n",
    "            df_true_train.apply(lambda x: spearmanr(x, df_prd.loc[x.name])[0], axis=1).mean()\n",
    "        )\n",
    "        metric = \"train coef: {:.4f}\".format(scores_train[-1])\n",
    "        if valid_idx is not None:\n",
    "            df_prd = pd.DataFrame(\n",
    "                target_proc.inverse_transform(m.predict(ds_valid, verbose = 0)), \n",
    "                index = df_true.index, columns = targets\n",
    "            )\n",
    "            scores_valid.append(\n",
    "                df_true.apply(lambda x: spearmanr(x, df_prd.loc[x.name])[0], axis=1).mean()\n",
    "            )\n",
    "            metric = metric + \", valid coef: {:.4f}\".format(scores_valid[-1])\n",
    "        progress_bar.set_postfix_str(metric)\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    return (scores_train, scores_valid), df_prd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c7d25-77b2-44c1-bf63-71e9de027b72",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0271b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 224\n",
    "img_height = 224\n",
    "df_spots = make_img_proc_info(df_spots, img_width, img_height)\n",
    "df_spots_test = make_img_proc_info(df_spots_test, img_width, img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0405540-a28d-4ece-a479-bc8ae991fe4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d48324d34840b4939c62ecae93501f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train 0:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf5f9d92fa14642aede341bfb33ebae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b708d81ae8534bc98ddcb79a668881b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83882e84a33c48a49e4775179aaa5d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b735e6d2e0546d999bbdb96386a8304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d73ed9676604a3b9d835d946f89c7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "gkf = GroupKFold(n_splits = 6)\n",
    "scores = list()\n",
    "oofs = list()\n",
    "for i, (train_idx, valid_idx) in enumerate(\n",
    "    gkf.split(df_spots[['x', 'y']], df_spots[targets], groups = df_spots['slide'])\n",
    "):\n",
    "    m, layers = create_model(img_width, img_height)\n",
    "    score_1, df_prd = train_model(\n",
    "        m, train_idx, valid_idx, learning_rate = 5e-7, \n",
    "        target_proc = target_proc, batch_size = 32, epochs = 10, step = 'train {}'.format(i)\n",
    "    )\n",
    "    \"\"\"\n",
    "    m = reconstruct_model(layers)\n",
    "    score_2, df_prd = train_model(\n",
    "        m, train_idx, valid_idx, learning_rate = 1e-6, \n",
    "        target_proc = target_proc, batch_size = 32, epochs = 10, step = 'fine tuning {}'.format(i)\n",
    "    )\n",
    "    \"\"\"\n",
    "    scores.append(score_1)\n",
    "    oofs.append(df_prd)\n",
    "df_oof = pd.concat(oofs, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d19d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['result/cv_log_std_pca_5_eff_b0.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((df_oof, scores), 'result/cv_log_std_pca_5_eff_b0.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab58473-e4a2-4138-9981-0ded6a2f548b",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a9b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 224\n",
    "img_height = 224\n",
    "df_spots = make_img_proc_info(df_spots, img_width, img_height)\n",
    "df_spots_test = make_img_proc_info(df_spots_test, img_width, img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c78e79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d860e9401e463fa960256e62f0d472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m, layers = create_model(img_width, img_height)\n",
    "score_1, df_prd = train_model(\n",
    "    m, df_spots.index, None, learning_rate = 5e-7, \n",
    "    target_proc = target_proc, batch_size = 32, epochs = 10, step = 'train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543df0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 75ms/step - loss: 0.8789 - mean_squared_error: 0.8789\n",
      "Epoch 2/10\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - loss: 0.8748 - mean_squared_error: 0.8748\n",
      "Epoch 3/10\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 76ms/step - loss: 0.8705 - mean_squared_error: 0.8705\n",
      "Epoch 4/10\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 73ms/step - loss: 0.8666 - mean_squared_error: 0.8666\n",
      "Epoch 5/10\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 74ms/step - loss: 0.8645 - mean_squared_error: 0.8645\n",
      "Epoch 6/10\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 83ms/step - loss: 0.8619 - mean_squared_error: 0.8619\n",
      "Epoch 7/10\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 73ms/step - loss: 0.8578 - mean_squared_error: 0.8578\n",
      "Epoch 8/10\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 73ms/step - loss: 0.8542 - mean_squared_error: 0.8542\n",
      "Epoch 9/10\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 95ms/step - loss: 0.8518 - mean_squared_error: 0.8518\n",
      "Epoch 10/10\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 83ms/step - loss: 0.8491 - mean_squared_error: 0.8491\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "inputs = tf.keras.Input(shape = input_shape)\n",
    "x = enet(inputs, training = True)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = d1(x)\n",
    "outputs = d2(x)\n",
    "m = tf.keras.models.Model(inputs, outputs)\n",
    "m.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-6),  # Low learning rate\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.metrics.MeanSquaredError()],\n",
    ")\n",
    "hist = m.fit(ds_train, epochs=10)\n",
    "\"\"\"\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8872b7f4-5a81-4cbe-bf0e-860c1fecae0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/log_std_pca_5_eff_b0.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(m.get_weights(), 'model/log_std_pca_5_eff_b0.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "324394a4-4a9d-4ac1-a7ab-e923c2c3b5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/target_proc_log_std_pca_5_eff_b0.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(target_proc, 'model/target_proc_log_std_pca_5_eff_b0.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03886d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
